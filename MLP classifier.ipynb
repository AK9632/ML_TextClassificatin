{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP classifier.ipynb","provenance":[],"collapsed_sections":["t_HnlwgKzJ0t","1y9LPHNMQBwx","2Onb7GuKzZ_a","35B6YO3G9jkA","bFa7iHdoPxiS","CRJPZOxOsoap"],"authorship_tag":"ABX9TyN78+aYKxBHuLSCvmx6V9k3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNI-DC5ImmpG","executionInfo":{"status":"ok","timestamp":1620586253215,"user_tz":-330,"elapsed":1260,"user":{"displayName":"Anvesh Kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOhw_iYLoY6AZqKmJEWDXO_3Yv2PYAk9BRNjsK8Q=s64","userId":"09289906135724588146"}},"outputId":"fd7370c1-d120-44ad-f631-70ad02d6d83d"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t_HnlwgKzJ0t"},"source":["# **PRE-PROCESSING**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yU3f3ScOwiOC","executionInfo":{"status":"ok","timestamp":1620586258866,"user_tz":-330,"elapsed":2850,"user":{"displayName":"Anvesh Kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOhw_iYLoY6AZqKmJEWDXO_3Yv2PYAk9BRNjsK8Q=s64","userId":"09289906135724588146"}},"outputId":"8031a134-ecae-4866-b601-9c55d63a0167"},"source":["############################################################# PREPROCESSING\n","\n","import numpy as np \n","import pandas as pd \n","import nltk\n","import string\n","\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop = stopwords.words('english')\n","\n","def remove_stopwords(text) :\n","  arr_of_words=text.split()\n","  return_text=''\n","  for word in arr_of_words :\n","    if word not in stop :\n","      return_text+=word\n","      return_text+=' '\n","  return return_text\n","\n","def remove_punctuations(text):\n","    for punctuation in string.punctuation:\n","        text = text.replace(punctuation, '')\n","    return text\n","\n","def make_lower(text) :\n","  lower_text=text.lower()\n","  return lower_text\n","\n","def remove_numbers(text) :\n","  alpha_text=''\n","  arr_of_words=text.split()\n","  for i in arr_of_words :\n","    try :\n","      x=int(i)\n","    except ValueError :\n","      alpha_text+=i\n","      alpha_text+=' '\n","  return alpha_text\n","\n","def deal_media(text) :\n","  arr_of_words=text.split()\n","  final_text=''\n","  for i in arr_of_words :\n","    if 'http://' in i :\n","      word='aszxdcfvgb'\n","    else :\n","      word=i\n","    final_text+=word\n","    final_text+=' '\n","  return final_text\n","\n","def remove_nan(text) :\n","  text = text.replace(' nan ', '')\n","  return text\n","\n","\n","\n","train_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/ML_Project/ML_data/train.csv\") \n","test_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/ML_Project/ML_data/test.csv\")\n","\n","#train_df = pd.read_csv(\"/content/drive/My Drive/ML_data/train.csv\") \n","#test_df = pd.read_csv(\"/content/drive/My Drive/ML_data/test.csv\")\n","\n","#adding location to text\n","train_df[\"text\"] = train_df[\"text\"] + ' ' + train_df[\"location\"].astype(str) +' '\n","test_df[\"text\"] = test_df[\"text\"] + ' ' + test_df[\"location\"].astype(str) +' '\n","\n","\n","train_df[\"text\"] = train_df[\"text\"].apply(remove_nan)\n","train_df[\"text\"] = train_df[\"text\"].apply(deal_media)\n","train_df[\"text\"] = train_df[\"text\"].apply(remove_stopwords)\n","train_df[\"text\"] = train_df[\"text\"].apply(remove_punctuations)\n","train_df[\"text\"] = train_df[\"text\"].apply(make_lower)\n","train_df[\"text\"] = train_df[\"text\"].apply(remove_numbers)\n","\n","test_df[\"text\"] = test_df[\"text\"].apply(remove_nan)\n","test_df[\"text\"] = test_df[\"text\"].apply(deal_media)\n","test_df[\"text\"] = test_df[\"text\"].apply(remove_stopwords)\n","test_df[\"text\"] = test_df[\"text\"].apply(remove_punctuations)\n","test_df[\"text\"] = test_df[\"text\"].apply(make_lower)\n","test_df[\"text\"] = test_df[\"text\"].apply(remove_numbers)\n","\n","\n","x_train=train_df[\"text\"]\n","y_train=train_df[\"target\"]\n","x_test=test_df[\"text\"]\n","\n","\n","print(\"Preprocessing done\")\n","print(\"\")\n","print(\"Locations added to text\")\n","print(\"Links were removed\")\n","print(\"Stopwords removed\")\n","print(\"punctuations removed\")\n","print(\"Numbers removed\")\n","print(\"Lowered the text\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Preprocessing done\n","\n","Locations added to text\n","Links were removed\n","Stopwords removed\n","punctuations removed\n","Numbers removed\n","Lowered the text\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1y9LPHNMQBwx"},"source":["# **VECTORIZATION**\n","\n"]},{"cell_type":"code","metadata":{"id":"AVyhzyYQQLCa"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","cv = CountVectorizer()\n","tv = TfidfVectorizer()\n","\n","x_traincv = cv.fit_transform(x_train)\n","x_traintv = tv.fit_transform(x_train)\n","\n","x_testcv = cv.transform(x_test)\n","x_testtv = tv.transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Onb7GuKzZ_a"},"source":["# **Word2Vec vectorizarion**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrJRKlP7nch9","executionInfo":{"status":"ok","timestamp":1620588926098,"user_tz":-330,"elapsed":106509,"user":{"displayName":"Anvesh Kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOhw_iYLoY6AZqKmJEWDXO_3Yv2PYAk9BRNjsK8Q=s64","userId":"09289906135724588146"}},"outputId":"7f443067-e6be-4c2e-fe55-98061b98c288"},"source":["# tried a large corpus, but failed\n","from tensorflow.keras.utils import get_file\n","\n","try:\n","    path = get_file('GoogleNews-vectors-negative300.bin.gz', origin='https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n","except:\n","    print('Error downloading')\n","    raise\n","    \n","print(path)\n","import gensim\n","model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","1647050752/1647046227 [==============================] - 103s 0us/step\n","/root/.keras/datasets/GoogleNews-vectors-negative300.bin.gz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7emZoy7dzh-0","executionInfo":{"status":"ok","timestamp":1620590026896,"user_tz":-330,"elapsed":12415,"user":{"displayName":"Anvesh Kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOhw_iYLoY6AZqKmJEWDXO_3Yv2PYAk9BRNjsK8Q=s64","userId":"09289906135724588146"}},"outputId":"63759dc5-acaf-467d-fc5c-3b6bb5f264de"},"source":["import pandas as pd\n","import nltk\n","nltk.download('punkt')\n","import gensim\n","from gensim import corpora, models, models, similarities\n","\n","\n","train_corpus=x_train\n","tok_train = [nltk.word_tokenize(sent) for sent in train_corpus]\n","test_corpus=x_test\n","tok_test = [nltk.word_tokenize(sent) for sent in test_corpus]\n","\n","\n","train_model = gensim.models.Word2Vec(tok_train, min_count=1, size=32)\n","test_model = gensim.models.Word2Vec(tok_test, min_count=1, size=32)\n","\n","#no_word_list=['ronge', 'sask', 'rockyfire', 'cafire', 'manitou', 'a', 'fvck', 'tampabay', 'goooooooaaaaaal','looooool', 'wayi', 'cooool', 'bbcmtd','aszxdcfvgb', 'africanbaze', 'newsnigeria']\n","\n","x_trainwv=[]\n","for sentence in tok_train :\n","  total=np.zeros((1, 32))\n","  total=total.tolist()\n","  total=total[0]\n","  total=np.array(total)\n","  len=0\n","  for word in sentence :\n","    #if word not in no_word_list:\n","    len+=1\n","    #vector_arr=train_model.wv[word].tolist()\n","    vector_arr=train_model.wv[word].tolist()\n","    vector_arr=np.array(vector_arr)\n","    total+= vector_arr\n","  total=total/len\n","  total=total.tolist()\n","  x_trainwv.append(total)\n","x_trainwv=np.array(x_trainwv)\n","\n","\n","\n","x_testwv=[]\n","for sentence in tok_test :\n","  total=np.zeros((1, 32))\n","  total=total.tolist()\n","  total=total[0]\n","  total=np.array(total)\n","  len=0\n","  for word in sentence :\n","    len+=1\n","    #vector_arr=test_model.wv[word].tolist()\n","    vector_arr=test_model.wv[word].tolist()\n","    vector_arr=np.array(vector_arr)\n","    total+= vector_arr\n","  total=total/len\n","  total=total.tolist()\n","  x_testwv.append(total)\n","x_testwv=np.array(x_testwv)\n","\n","\n","\n","\n","print(\"Vectorization done for train and test data\")\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Vectorization done for train and test data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"35B6YO3G9jkA"},"source":["# **MULTI LAYER PERCEPTRON**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1RR3mhXM9sPo","executionInfo":{"status":"ok","timestamp":1620586729268,"user_tz":-330,"elapsed":385475,"user":{"displayName":"Anvesh Kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOhw_iYLoY6AZqKmJEWDXO_3Yv2PYAk9BRNjsK8Q=s64","userId":"09289906135724588146"}},"outputId":"4444d336-ff47-4d4c-b1c3-a46e2e1547cf"},"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import accuracy_score\n","classifier = MLPClassifier()\n","\n","\n","list_of_parameters_for_hidden_layer_sizes=[(64, 64,), (128, 64,)]\n","list_of_parameters_for_activation=['logistic', 'relu']\n","list_of_parameters_for_solver=['sgd', 'adam']\n","list_of_parameters_for_learning_rate_init=[0.001, 0.002]\n","list_of_parameters_for_max_iter=[500]\n","\n","\n","params={'hidden_layer_sizes':list_of_parameters_for_hidden_layer_sizes, 'activation':list_of_parameters_for_activation, 'solver':list_of_parameters_for_solver, 'learning_rate_init':list_of_parameters_for_learning_rate_init, 'max_iter':list_of_parameters_for_max_iter}\n","gs=GridSearchCV(estimator=classifier, param_grid=params, cv=5)\n","\n","\n","gs = gs.fit(x_trainwv, y_train)\n","best_params=gs.best_params_\n","accuracy=gs.best_score_\n","print(\"For Multi-layer perceptron :\")\n","print(\"Best hyperparameter :\", best_params)\n","print(\"Accuracy of train data :\", accuracy)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["For Multi-layer perceptron :\n","Best hyperparameter : {'activation': 'relu', 'hidden_layer_sizes': (64, 64), 'learning_rate_init': 0.001, 'max_iter': 500, 'solver': 'adam'}\n","Accuracy of train data : 0.6369388172420607\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bFa7iHdoPxiS"},"source":["# **ACCURACY METRICS**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJVvPlTpP2rN","executionInfo":{"status":"ok","timestamp":1620633294829,"user_tz":-330,"elapsed":988,"user":{"displayName":"Anvesh Kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOhw_iYLoY6AZqKmJEWDXO_3Yv2PYAk9BRNjsK8Q=s64","userId":"09289906135724588146"}},"outputId":"553501aa-319a-4b86-9e92-3772745a92f3"},"source":["\n","from sklearn.model_selection import cross_val_predict\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","clf = MLPClassifier(hidden_layer_sizes=(64, 64,), activation='relu', solver='adam', learning_rate_init=0.001, max_iter=500)\n","\n","# Count Vectorization\n","y_pred = cross_val_predict(clf, x_traincv, y_train, cv=5)\n","accuracy = accuracy_score(y_train, y_pred)\n","print(\"Count Vectorization :\")\n","print(\"Accuracy :\", accuracy)\n","f1 = f1_score(y_train, y_pred)\n","print(\"F1 Score :\", f1)\n","conf_mat = confusion_matrix(y_train, y_pred)\n","print(\"Confusion Matrix :\")\n","print(conf_mat)\n","print(\"\")\n","\n","\n","#TFID Vectorization\n","y_pred = cross_val_predict(clf, x_traintv, y_train, cv=5)\n","accuracy = accuracy_score(y_train, y_pred)\n","print(\"TFID Vectorization :\")\n","print(\"Accuracy :\", accuracy)\n","f1 = f1_score(y_train, y_pred)\n","print(\"F1 Score :\", f1)\n","conf_mat = confusion_matrix(y_train, y_pred)\n","print(\"Confusion Matrix :\")\n","print(conf_mat)\n","print(\"\")\n","\n","\n","#Word2Vec vectorization\n","y_pred = cross_val_predict(clf, x_trainwv, y_train, cv=5)\n","accuracy = accuracy_score(y_train, y_pred)\n","print(\"Word2Vec Vectorization :\")\n","print(\"Accuracy :\", accuracy)\n","f1 = f1_score(y_train, y_pred)\n","print(\"F1 Score :\", f1)\n","conf_mat = confusion_matrix(y_train, y_pred)\n","print(\"Confusion Matrix :\")\n","print(conf_mat)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Count Vectorization :\n","Accuracy : 0.7267713122290818\n","F1 Score : 0.6434339505212386\n","Confusion Matrix :\n","[[2931 1411]\n"," [1273 1998]]\n","\n","TFID Vectorization :\n","Accuracy : 0.7374451595954289\n","F1 Score : 0.6582035928143714\n","Confusion Matrix :\n","[[3093 1249]\n"," [1364 1907]]\n","\n","Word2Vec Vectorization :\n","Accuracy : 0.6891869171154604\n","F1 Score : 0.5486680873734684\n","Confusion Matrix :\n","[[3386  956]\n"," [1867 1404]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CRJPZOxOsoap"},"source":["# **On Test Data**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFsOlLi4suIl","executionInfo":{"status":"ok","timestamp":1620590893662,"user_tz":-330,"elapsed":215788,"user":{"displayName":"Anvesh Kiran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjOhw_iYLoY6AZqKmJEWDXO_3Yv2PYAk9BRNjsK8Q=s64","userId":"09289906135724588146"}},"outputId":"9db51e95-6625-41d6-afc4-917c7a0c9435"},"source":["import pandas as pd\n","\n","clf = MLPClassifier(hidden_layer_sizes=(64, 64,), activation='relu', solver='adam', learning_rate_init=0.001, max_iter=500)\n","\n","#Count Vectorization\n","clf.fit(x_traincv, y_train)\n","y_pred=list(clf.predict(x_testcv))\n","id_arr=list(test_df[\"id\"])\n","target_arr=y_pred\n","df = pd.DataFrame(list(zip(id_arr, target_arr)),columns =['id', 'target'])\n","df.to_csv('Submission_file_mlp_cv.csv', index=False)\n","!cp Submission_file_mlp_cv.csv \"drive/My Drive/\"\n","print(\"cv file made\")\n","#f1 score is 0.74869\n","\n","#TFID Vectorization\n","clf.fit(x_traintv, y_train)\n","y_pred=list(clf.predict(x_testtv))\n","id_arr=list(test_df[\"id\"])\n","target_arr=y_pred\n","df = pd.DataFrame(list(zip(id_arr, target_arr)),columns =['id', 'target'])\n","df.to_csv('Submission_file_mlp_tv.csv', index=False)\n","!cp Submission_file_mlp_tv.csv \"drive/My Drive/\"\n","print(\"tv file made\")\n","#f1 score is 0.74072\n","\n","#Word2Vec Vectorization\n","clf.fit(x_trainwv, y_train)\n","y_pred=list(clf.predict(x_testwv))\n","id_arr=list(test_df[\"id\"])\n","target_arr=y_pred\n","df = pd.DataFrame(list(zip(id_arr, target_arr)),columns =['id', 'target'])\n","df.to_csv('Submission_file_mlp_wv.csv', index=False)\n","!cp Submission_file_mlp_wv.csv \"drive/My Drive/\"\n","print(\"wv file made\")\n","#f1 score is 0.58075"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cv file made\n","tv file made\n","wv file made\n"],"name":"stdout"}]}]}